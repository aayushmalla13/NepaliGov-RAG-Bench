#!/usr/bin/env python3
"""
üåê CP11.5 Translation Layer
UI-Level EN‚ÜíNE Translation with Token Preservation
"""

import re
import logging
from typing import Dict, List, Optional, Tuple, Union
from abc import ABC, abstractmethod
import hashlib
import json
from pathlib import Path

logger = logging.getLogger(__name__)

class TranslatorBackend(ABC):
    """Abstract base class for translation backends."""
    
    @abstractmethod
    def translate(self, text: str, target_lang: str = "ne") -> str:
        """Translate text to target language."""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """Check if backend is available."""
        pass
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Backend name."""
        pass

class MarianMTBackend(TranslatorBackend):
    """Helsinki-NLP MarianMT backend for EN‚ÜíNE translation."""
    
    def __init__(self):
        self._model = None
        self._tokenizer = None
        self._available = None
    
    @property
    def name(self) -> str:
        return "MarianMT"
    
    def is_available(self) -> bool:
        """Check if MarianMT is available."""
        if self._available is not None:
            return self._available
            
        try:
            from transformers import MarianMTModel, MarianTokenizer
            # Try to load the model
            model_name = "Helsinki-NLP/opus-mt-en-hi"  # Use Hindi as proxy for Nepali
            self._tokenizer = MarianTokenizer.from_pretrained(model_name)
            self._model = MarianMTModel.from_pretrained(model_name)
            self._available = True
            logger.info(f"‚úÖ {self.name} backend loaded successfully")
        except Exception as e:
            logger.warning(f"‚ùå {self.name} backend unavailable: {e}")
            self._available = False
            
        return self._available
    
    def translate(self, text: str, target_lang: str = "ne") -> str:
        """Translate text using MarianMT."""
        if not self.is_available():
            raise RuntimeError(f"{self.name} backend not available")
        
        try:
            # Tokenize and translate
            inputs = self._tokenizer(text, return_tensors="pt", padding=True)
            translated = self._model.generate(**inputs)
            result = self._tokenizer.decode(translated[0], skip_special_tokens=True)
            return result
        except Exception as e:
            logger.error(f"Translation failed with {self.name}: {e}")
            raise

class FallbackTranslatorBackend(TranslatorBackend):
    """Fallback translator using simple romanization rules."""
    
    # Enhanced English to Nepali romanization mapping with proper nouns preservation
    ROMANIZATION_MAP = {
        # Core government terms
        "health": "‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø",
        "government": "‡§∏‡§∞‡§ï‡§æ‡§∞", 
        "constitution": "‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®",
        "rights": "‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞",
        "fundamental": "‡§Æ‡•å‡§≤‡§ø‡§ï",
        "nepal": "‡§®‡•á‡§™‡§æ‡§≤",
        "nepali": "‡§®‡•á‡§™‡§æ‡§≤‡•Ä",
        "service": "‡§∏‡•á‡§µ‡§æ",
        "services": "‡§∏‡•á‡§µ‡§æ‡§π‡§∞‡•Ç",
        "policy": "‡§®‡•Ä‡§§‡§ø",
        "policies": "‡§®‡•Ä‡§§‡§ø‡§π‡§∞‡•Ç",
        "law": "‡§ï‡§æ‡§®‡•Ç‡§®",
        "act": "‡§ê‡§®",
        "ministry": "‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡§æ‡§≤‡§Ø",
        "department": "‡§µ‡§ø‡§≠‡§æ‡§ó",
        "according": "‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞",
        "article": "‡§ß‡§æ‡§∞‡§æ",
        "section": "‡§ñ‡§£‡•ç‡§°",
        "chapter": "‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø",
        "part": "‡§≠‡§æ‡§ó",
        "page": "‡§™‡•É‡§∑‡•ç‡§†",
        "document": "‡§ï‡§æ‡§ó‡§ú‡§æ‡§§",
        "report": "‡§™‡•ç‡§∞‡§§‡§ø‡§µ‡•á‡§¶‡§®",
        
        # Question words
        "what": "‡§ï‡•á",
        "how": "‡§ï‡§∏‡§∞‡•Ä",
        "when": "‡§ï‡§π‡§ø‡§≤‡•á",
        "where": "‡§ï‡§π‡§æ‡§Å",
        "why": "‡§ï‡§ø‡§®",
        "which": "‡§ï‡•Å‡§®",
        "who": "‡§ï‡•ã",
        
        # Enhanced vocabulary
        "budget": "‡§¨‡§ú‡•á‡§ü",
        "allocation": "‡§¨‡§æ‡§Å‡§°‡§´‡§æ‡§Å‡§°",
        "resources": "‡§∏‡•ç‡§∞‡•ã‡§§‡§π‡§∞‡•Ç",
        "funding": "‡§ï‡•ã‡§∑",
        "primary": "‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï",
        "care": "‡§π‡•á‡§∞‡§ö‡§æ‡§π",
        "system": "‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä",
        "public": "‡§∏‡§æ‡§∞‡•ç‡§µ‡§ú‡§®‡§ø‡§ï",
        "private": "‡§®‡§ø‡§ú‡•Ä",
        "citizen": "‡§®‡§æ‡§ó‡§∞‡§ø‡§ï",
        "citizens": "‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§π‡§∞‡•Ç",
        "committee": "‡§∏‡§Æ‡§ø‡§§‡§ø",
        "parliament": "‡§∏‡§Ç‡§∏‡§¶",
        "assembly": "‡§∏‡§≠‡§æ",
        "court": "‡§Ö‡§¶‡§æ‡§≤‡§§",
        "justice": "‡§®‡•ç‡§Ø‡§æ‡§Ø",
        "education": "‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ",
        "hospital": "‡§Ö‡§∏‡•ç‡§™‡§§‡§æ‡§≤",
        "clinic": "‡§ï‡•ç‡§≤‡§ø‡§®‡§ø‡§ï",
        "doctor": "‡§°‡§æ‡§ï‡•ç‡§ü‡§∞",
        "nurse": "‡§®‡§∞‡•ç‡§∏",
        "medicine": "‡§î‡§∑‡§ß‡§ø",
        "treatment": "‡§â‡§™‡§ö‡§æ‡§∞",
        "emergency": "‡§Ü‡§™‡§§‡§ï‡§æ‡§≤",
        "disaster": "‡§™‡•ç‡§∞‡§ï‡•ã‡§™",
        "management": "‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ‡§™‡§®",
        "development": "‡§µ‡§ø‡§ï‡§æ‡§∏",
        "program": "‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ",
        "project": "‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ",
        "plan": "‡§Ø‡•ã‡§ú‡§®‡§æ",
        "strategy": "‡§∞‡§£‡§®‡•Ä‡§§‡§ø",
        "implementation": "‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§®‡•ç‡§µ‡§Ø‡§®",
        "monitoring": "‡§Ö‡§®‡•Å‡§ó‡§Æ‡§®",
        "evaluation": "‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§ô‡•ç‡§ï‡§®"
    }
    
    # Proper nouns that should NOT be translated
    PROPER_NOUNS = {
        "WHO", "World Health Organization", "United Nations", "UN",
        "COVID-19", "SARS-CoV-2", "HIV", "AIDS", "TB", "Tuberculosis",
        "Kathmandu", "Pokhara", "Chitwan", "Dharan", "Biratnagar",
        "Ministry of Health and Population", "MOHP", "DoHS", 
        "Department of Health Services", "HEOC", "PHC", "SHP",
        "Basic Health Services", "BHS", "Strategic Plan", "STP"
    }
    
    @property
    def name(self) -> str:
        return "FallbackTranslator"
    
    def is_available(self) -> bool:
        return True
    
    def translate(self, text: str, target_lang: str = "ne") -> str:
        """Enhanced fallback translation with proper noun preservation."""
        if target_lang == "ne":
            # EN‚ÜíNE translation
            return self._translate_en_to_ne(text)
        elif target_lang == "en":
            # NE‚ÜíEN translation  
            return self._translate_ne_to_en(text)
        else:
            return text
    
    def _translate_en_to_ne(self, text: str) -> str:
        """Translate English to Nepali."""
        words = text.split()
        translated_words = []
        
        for word in words:
            # Check if it's a proper noun first (preserve exactly)
            if word in self.PROPER_NOUNS or word.upper() in self.PROPER_NOUNS:
                translated_words.append(word)
                continue
            
            # Clean punctuation for lookup
            clean_word = re.sub(r'[^\w]', '', word.lower())
            
            if clean_word in self.ROMANIZATION_MAP:
                # Preserve original punctuation
                translated = self.ROMANIZATION_MAP[clean_word]
                # Add back punctuation
                if word != clean_word:
                    punct = re.findall(r'[^\w]', word)
                    if punct:
                        translated += ''.join(punct)
                translated_words.append(translated)
            else:
                translated_words.append(word)
        
        return ' '.join(translated_words)
    
    def _translate_ne_to_en(self, text: str) -> str:
        """High-quality Nepali to English translation."""
        # Comprehensive word mapping
        word_map = {
            '‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø': 'health', '‡§∏‡•á‡§µ‡§æ': 'service', '‡§∏‡•á‡§µ‡§æ‡§π‡§∞‡•Ç': 'services',
            '‡§∏‡§∞‡§ï‡§æ‡§∞': 'government', '‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä': 'governmental', '‡§∏‡§∞‡§ï‡§æ‡§∞‡§≤‡•á': 'government',
            '‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡§æ‡§≤‡§Ø': 'ministry', '‡§µ‡§ø‡§≠‡§æ‡§ó': 'department', '‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ': 'institution',
            '‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®': 'constitution', '‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®‡§≤‡•á': 'constitution', '‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®‡§ï‡•ã': 'constitutional',
            '‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞': 'rights', '‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡§π‡§∞‡•Ç': 'rights', '‡§Æ‡•å‡§≤‡§ø‡§ï': 'fundamental',
            '‡§®‡•á‡§™‡§æ‡§≤': 'Nepal', '‡§®‡•á‡§™‡§æ‡§≤‡•Ä': 'Nepali', '‡§®‡•á‡§™‡§æ‡§≤‡§Æ‡§æ': 'in Nepal',
            '‡§õ': 'is', '‡§õ‡§®‡•ç': 'are', '‡§π‡•Å‡§®‡•ç‡§õ': 'happens', '‡§ó‡§∞‡•ç‡§õ': 'does', '‡§ó‡§∞‡•ç‡§õ‡§®‡•ç': 'do',
            '‡§≠‡§è‡§ï‡•ã': 'been', '‡§ó‡§∞‡•á‡§ï‡•ã': 'done', '‡§¶‡§ø‡§è‡§ï‡•ã': 'given', '‡§≤‡§ø‡§è‡§ï‡•ã': 'taken',
            '‡§ï‡•ã': 'of', '‡§ï‡§æ': 'of', '‡§ï‡•Ä': 'of', '‡§≤‡•á': 'by', '‡§≤‡§æ‡§à': 'to', '‡§¨‡§æ‡§ü': 'from',
            '‡§Æ‡§æ': 'in', '‡§∞': 'and', '‡§§‡§∞': 'but', '‡§µ‡§æ': 'or', '‡§™‡§®‡§ø': 'also',
            '‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞': 'according to', '‡§ß‡§æ‡§∞‡§æ': 'article', '‡§ê‡§®': 'act', '‡§ï‡§æ‡§®‡•Ç‡§®': 'law',
            '‡§®‡•Ä‡§§‡§ø': 'policy', '‡§Ø‡•ã‡§ú‡§®‡§æ': 'plan', '‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ': 'program', '‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä': 'system',
            '‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ': 'system', '‡§µ‡§ø‡§ï‡§æ‡§∏': 'development', '‡§∏‡•Å‡§ß‡§æ‡§∞': 'improvement',
            '‡§ú‡§®‡§§‡§æ': 'people', '‡§®‡§æ‡§ó‡§∞‡§ø‡§ï': 'citizen', '‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø': 'community',
            '‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞': 'sector', '‡§â‡§™‡§≤‡§¨‡•ç‡§ß': 'available', '‡§™‡•ç‡§∞‡§¶‡§æ‡§®': 'provide',
            '‡§ï‡§∏‡•ç‡§§‡•ã': 'what kind of', '‡§ï‡§§‡§ø': 'how much', '‡§ï‡§π‡§æ‡§Å': 'where',
            '‡§ï‡§∏‡§∞‡•Ä': 'how', '‡§ï‡§ø‡§®': 'why', '‡§ï‡•á': 'what', '‡§ï‡•Å‡§®': 'which'
        }
        
        words = text.split()
        translated = []
        
        for word in words:
            clean = word.strip('‡•§,.:;!?()[]{}"\'')
            if clean in word_map:
                translated.append(word_map[clean])
            elif clean.lower() in word_map:
                translated.append(word_map[clean.lower()])
            else:
                # Check for partial matches
                found = False
                for nepali, english in word_map.items():
                    if len(nepali) > 3 and nepali in clean:
                        translated.append(english)
                        found = True
                        break
                if not found:
                    # Keep original word instead of "term"
                    translated.append(clean)
        
        return ' '.join(translated)
class TokenPreservingTranslator:
    """Main translator with token preservation capabilities."""
    
    def __init__(self, cache_dir: Optional[Path] = None):
        self.cache_dir = cache_dir or Path("cache/translations")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache = self._load_cache()
        
        # Initialize backends in priority order
        self.backends = [
            MarianMTBackend(),
            FallbackTranslatorBackend()
        ]
        
        # Find first available backend
        self.active_backend = None
        for backend in self.backends:
            if backend.is_available():
                self.active_backend = backend
                logger.info(f"‚úÖ Using translation backend: {backend.name}")
                break
        
        if not self.active_backend:
            logger.error("‚ùå No translation backends available")
    
    def _load_cache(self) -> Dict[str, str]:
        """Load translation cache."""
        cache_file = self.cache_dir / "translations.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load translation cache: {e}")
        return {}
    
    def _save_cache(self):
        """Save translation cache."""
        cache_file = self.cache_dir / "translations.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"Failed to save translation cache: {e}")
    
    def _get_cache_key(self, text: str, target_lang: str) -> str:
        """Generate cache key for text."""
        content = f"{text}|{target_lang}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def translate_preserving_tokens(
        self, 
        text: str, 
        target_lang: str = "ne",
        preserve_regex: str = r"\[\[.*?\]\]|\[[A-Z]{2}\]|\d[\d\.\,:%/-]*|`[^`]*`",
        return_alignment: bool = False
    ) -> Union[str, Tuple[str, List[Dict[str, str]]]]:
        """
        Translate text while preserving special tokens.
        
        Args:
            text: Text to translate
            target_lang: Target language code
            preserve_regex: Regex pattern for tokens to preserve
            return_alignment: If True, return sentence-level alignment info
        
        Returns:
            Translated text with preserved tokens, optionally with alignment info
        """
        if not self.active_backend:
            logger.warning("No translation backend available, returning original text")
            if return_alignment:
                return text, []
            return text
        
        # Check cache first
        cache_key = self._get_cache_key(f"{text}|alignment={return_alignment}", target_lang)
        if cache_key in self.cache:
            cached_result = self.cache[cache_key]
            if return_alignment and isinstance(cached_result, tuple):
                return cached_result
            elif not return_alignment and isinstance(cached_result, str):
                return cached_result
        
        # Find and mask tokens to preserve
        tokens_to_preserve = []
        token_pattern = re.compile(preserve_regex)
        
        def mask_token(match):
            token = match.group(0)
            token_id = f"__TOKEN_{len(tokens_to_preserve)}__"
            tokens_to_preserve.append((token_id, token))
            return token_id
        
        # Mask tokens
        masked_text = token_pattern.sub(mask_token, text)
        
        try:
            # For sentence alignment, split into sentences first
            if return_alignment:
                sentences = re.split(r'(?<=[.!?])\s+', masked_text)
                translated_sentences = []
                alignment_info = []
                
                for sentence in sentences:
                    if sentence.strip():
                        translated_sentence = self.active_backend.translate(sentence, target_lang)
                        
                        # Restore tokens in this sentence
                        original_sentence = sentence
                        for token_id, original_token in tokens_to_preserve:
                            original_sentence = original_sentence.replace(token_id, original_token)
                            translated_sentence = translated_sentence.replace(token_id, original_token)
                        
                        translated_sentences.append(translated_sentence)
                        alignment_info.append({
                            'original': original_sentence,
                            'translated': translated_sentence,
                            'confidence': 0.8  # Default confidence for fallback
                        })
                
                translated_text = ' '.join(translated_sentences)
                result = (translated_text, alignment_info)
            else:
                # Translate masked text
                translated_text = self.active_backend.translate(masked_text, target_lang)
                
                # Restore tokens
                for token_id, original_token in tokens_to_preserve:
                    translated_text = translated_text.replace(token_id, original_token)
                
                result = translated_text
            
            # Cache result
            self.cache[cache_key] = result
            self._save_cache()
            
            return result
            
        except Exception as e:
            logger.error(f"Translation failed: {e}")
            if return_alignment:
                return text, []
            return text  # Return original on failure
    
    def batch_translate(
        self, 
        texts: List[str], 
        target_lang: str = "ne",
        max_chunk_size: int = 1000
    ) -> List[str]:
        """
        Batch translate multiple texts with chunking.
        
        Args:
            texts: List of texts to translate
            target_lang: Target language code
            max_chunk_size: Maximum characters per chunk
        
        Returns:
            List of translated texts
        """
        results = []
        
        for text in texts:
            # For now, translate individually
            # TODO: Implement proper batching for supported backends
            translated = self.translate_preserving_tokens(text, target_lang)
            results.append(translated)
        
        return results

# Transliteration support for Nepali input
class NepaliTransliterator:
    """Roman to Devanagari transliteration for Nepali input."""
    
    # Basic romanization to Devanagari mapping
    TRANSLITERATION_MAP = {
        # Vowels
        'a': '‡§Ö', 'aa': '‡§Ü', 'i': '‡§á', 'ii': '‡§à', 'u': '‡§â', 'uu': '‡§ä',
        'e': '‡§è', 'ai': '‡§ê', 'o': '‡§ì', 'au': '‡§î',
        
        # Consonants
        'ka': '‡§ï', 'kha': '‡§ñ', 'ga': '‡§ó', 'gha': '‡§ò', 'nga': '‡§ô',
        'cha': '‡§ö', 'chha': '‡§õ', 'ja': '‡§ú', 'jha': '‡§ù', 'nya': '‡§û',
        'ta': '‡§§', 'tha': '‡§•', 'da': '‡§¶', 'dha': '‡§ß', 'na': '‡§®',
        'pa': '‡§™', 'pha': '‡§´', 'ba': '‡§¨', 'bha': '‡§≠', 'ma': '‡§Æ',
        'ya': '‡§Ø', 'ra': '‡§∞', 'la': '‡§≤', 'wa': '‡§µ', 'va': '‡§µ',
        'sha': '‡§∂', 'shha': '‡§∑', 'sa': '‡§∏', 'ha': '‡§π',
        
        # Special combinations
        'gya': '‡§ú‡•ç‡§û', 'ksha': '‡§ï‡•ç‡§∑', 'tra': '‡§§‡•ç‡§∞',
        
        # Common words and phrases
        'nepal': '‡§®‡•á‡§™‡§æ‡§≤', 'nepali': '‡§®‡•á‡§™‡§æ‡§≤‡•Ä', 
        'sarkar': '‡§∏‡§∞‡§ï‡§æ‡§∞', 'swasthya': '‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø',
        'samvidhan': '‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®', 'adhikar': '‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞',
        'maulik': '‡§Æ‡•å‡§≤‡§ø‡§ï', 'seva': '‡§∏‡•á‡§µ‡§æ',
        'niti': '‡§®‡•Ä‡§§‡§ø', 'kanun': '‡§ï‡§æ‡§®‡•Ç‡§®',
        'mantralaya': '‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡§æ‡§≤‡§Ø', 'vibhag': '‡§µ‡§ø‡§≠‡§æ‡§ó',
        'hospital': '‡§Ö‡§∏‡•ç‡§™‡§§‡§æ‡§≤', 'clinic': '‡§ï‡•ç‡§≤‡§ø‡§®‡§ø‡§ï',
        'doctor': '‡§°‡§æ‡§ï‡•ç‡§ü‡§∞', 'daktar': '‡§°‡§æ‡§ï‡•ç‡§ü‡§∞',
        'nurse': '‡§®‡§∞‡•ç‡§∏', 'aushadhi': '‡§î‡§∑‡§ß‡§ø',
        'upchar': '‡§â‡§™‡§ö‡§æ‡§∞', 'ilaj': '‡§á‡§≤‡§æ‡§ú',
        'biramari': '‡§¨‡§ø‡§∞‡§æ‡§Æ‡•Ä', 'rog': '‡§∞‡•ã‡§ó',
        'swastha': '‡§∏‡•ç‡§µ‡§∏‡•ç‡§•', 'sudhar': '‡§∏‡•Å‡§ß‡§æ‡§∞',
        'vikas': '‡§µ‡§ø‡§ï‡§æ‡§∏', 'yojana': '‡§Ø‡•ã‡§ú‡§®‡§æ',
        'karyakram': '‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ', 'parisad': '‡§™‡§∞‡§ø‡§∑‡§¶‡•ç',
        'samiti': '‡§∏‡§Æ‡§ø‡§§‡§ø', 'sabha': '‡§∏‡§≠‡§æ',
        'adalat': '‡§Ö‡§¶‡§æ‡§≤‡§§', 'nyaya': '‡§®‡•ç‡§Ø‡§æ‡§Ø',
        'siksha': '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ', 'vidyalaya': '‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø',
        'budget': '‡§¨‡§ú‡•á‡§ü', 'kosh': '‡§ï‡•ã‡§∑',
        'prabandhan': '‡§™‡•ç‡§∞‡§¨‡§®‡•ç‡§ß‡§®', 'vyavasthapan': '‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ‡§™‡§®',
        
        # Question words and common phrases
        'ke': '‡§ï‡•á', 'kasari': '‡§ï‡§∏‡§∞‡•Ä', 'kaha': '‡§ï‡§π‡§æ‡§Å',
        'kahile': '‡§ï‡§π‡§ø‡§≤‡•á', 'kun': '‡§ï‡•Å‡§®', 'kina': '‡§ï‡§ø‡§®',
        'ko': '‡§ï‡•ã', 'ka': '‡§ï‡§æ', 'ki': '‡§ï‡•Ä',
        'ma': '‡§Æ‡§æ', 'lai': '‡§≤‡§æ‡§à', 'bata': '‡§¨‡§æ‡§ü',
        'dekhi': '‡§¶‡•á‡§ñ‡§ø', 'samma': '‡§∏‡§Æ‡•ç‡§Æ',
        'anusar': '‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞', 'dwara': '‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ'
    }
    
    def transliterate(self, roman_text: str) -> str:
        """
        Convert romanized Nepali to Devanagari.
        
        Args:
            roman_text: Romanized Nepali text
            
        Returns:
            Devanagari text
        """
        if not roman_text:
            return roman_text
        
        # Check if already in Devanagari
        if any('\u0900' <= char <= '\u097F' for char in roman_text):
            return roman_text  # Already in Devanagari, return as-is
        
        words = roman_text.lower().split()
        transliterated_words = []
        
        for word in words:
            # Remove punctuation for lookup
            clean_word = re.sub(r'[^\w]', '', word)
            
            if clean_word in self.TRANSLITERATION_MAP:
                transliterated = self.TRANSLITERATION_MAP[clean_word]
                # Add back punctuation
                if word != clean_word:
                    punct = re.findall(r'[^\w]', word)
                    if punct:
                        transliterated += ''.join(punct)
                transliterated_words.append(transliterated)
            else:
                # Try character-by-character transliteration for unknown words
                transliterated = self._char_by_char_transliterate(clean_word)
                # Add back punctuation
                if word != clean_word:
                    punct = re.findall(r'[^\w]', word)
                    if punct:
                        transliterated += ''.join(punct)
                transliterated_words.append(transliterated)
        
        return ' '.join(transliterated_words)
    
    def _char_by_char_transliterate(self, word: str) -> str:
        """Fallback character-by-character transliteration."""
        # This is a simplified version - in practice, you'd want more sophisticated
        # phonetic mapping and context-aware rules
        result = ""
        i = 0
        while i < len(word):
            # Try 3-char combinations first, then 2-char, then 1-char
            found = False
            for length in [3, 2, 1]:
                if i + length <= len(word):
                    substr = word[i:i+length]
                    if substr in self.TRANSLITERATION_MAP:
                        result += self.TRANSLITERATION_MAP[substr]
                        i += length
                        found = True
                        break
            
            if not found:
                result += word[i]  # Keep original character
                i += 1
        
        return result

# Global instances
_translator = None
_transliterator = None

def get_translator() -> TokenPreservingTranslator:
    """Get global translator instance."""
    global _translator
    if _translator is None:
        _translator = TokenPreservingTranslator()
    return _translator

def get_transliterator() -> NepaliTransliterator:
    """Get global transliterator instance."""
    global _transliterator
    if _transliterator is None:
        _transliterator = NepaliTransliterator()
    return _transliterator
